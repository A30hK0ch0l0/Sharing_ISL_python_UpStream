{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Lab: Logistic Regression, LDA, QDA, and KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.1 The Stock Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import math\n",
    "from patsy import dmatrices\n",
    "\n",
    "\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Smarket = pd.read_csv('data/Smarket.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction\n",
       "0  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up\n",
       "1  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032        Up\n",
       "2  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623      Down\n",
       "3  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614        Up\n",
       "4  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213        Up"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Lag1',\n",
       " 'Lag2',\n",
       " 'Lag3',\n",
       " 'Lag4',\n",
       " 'Lag5',\n",
       " 'Volume',\n",
       " 'Today',\n",
       " 'Direction']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 9)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For panda data frame, there is a method corr to compute pairwise correlation between numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.030596</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.035689</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.030095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>0.029700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.026155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>0.030596</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.010250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag3</th>\n",
       "      <td>0.033195</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.002448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag4</th>\n",
       "      <td>0.035689</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag5</th>\n",
       "      <td>0.029788</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>-0.034860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Today</th>\n",
       "      <td>0.030095</td>\n",
       "      <td>-0.026155</td>\n",
       "      <td>-0.010250</td>\n",
       "      <td>-0.002448</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>-0.034860</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
       "Year    1.000000  0.029700  0.030596  0.033195  0.035689  0.029788  0.539006   \n",
       "Lag1    0.029700  1.000000 -0.026294 -0.010803 -0.002986 -0.005675  0.040910   \n",
       "Lag2    0.030596 -0.026294  1.000000 -0.025897 -0.010854 -0.003558 -0.043383   \n",
       "Lag3    0.033195 -0.010803 -0.025897  1.000000 -0.024051 -0.018808 -0.041824   \n",
       "Lag4    0.035689 -0.002986 -0.010854 -0.024051  1.000000 -0.027084 -0.048414   \n",
       "Lag5    0.029788 -0.005675 -0.003558 -0.018808 -0.027084  1.000000 -0.022002   \n",
       "Volume  0.539006  0.040910 -0.043383 -0.041824 -0.048414 -0.022002  1.000000   \n",
       "Today   0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860  0.014592   \n",
       "\n",
       "           Today  \n",
       "Year    0.030095  \n",
       "Lag1   -0.026155  \n",
       "Lag2   -0.010250  \n",
       "Lag3   -0.002448  \n",
       "Lag4   -0.006900  \n",
       "Lag5   -0.034860  \n",
       "Volume  0.014592  \n",
       "Today   1.000000  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As one would expect, the correlations between the lag variables and today’s returns are close to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b544950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Smarket.ix[:, 6])\n",
    "#plt.plot(Smarket[['Volume']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is some known complications that in Sklearn about applying parameter regularization. This can be aviod to set the tuning parameter 'C' to a large number. Here to be consistent with R output, I decieded to use Statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Direction[Down]  Direction[Up]\n",
      "0                 0.0            1.0\n",
      "1                 0.0            1.0\n",
      "2                 1.0            0.0\n",
      "3                 0.0            1.0\n",
      "4                 0.0            1.0\n",
      "5                 0.0            1.0\n",
      "6                 1.0            0.0\n",
      "7                 0.0            1.0\n",
      "8                 0.0            1.0\n",
      "9                 0.0            1.0\n",
      "10                1.0            0.0\n",
      "11                1.0            0.0\n",
      "12                0.0            1.0\n",
      "13                0.0            1.0\n",
      "14                1.0            0.0\n",
      "15                0.0            1.0\n",
      "16                1.0            0.0\n",
      "17                0.0            1.0\n",
      "18                1.0            0.0\n",
      "19                1.0            0.0\n",
      "20                1.0            0.0\n",
      "21                1.0            0.0\n",
      "22                0.0            1.0\n",
      "23                1.0            0.0\n",
      "24                1.0            0.0\n",
      "25                0.0            1.0\n",
      "26                1.0            0.0\n",
      "27                1.0            0.0\n",
      "28                1.0            0.0\n",
      "29                1.0            0.0\n",
      "...               ...            ...\n",
      "1220              0.0            1.0\n",
      "1221              0.0            1.0\n",
      "1222              0.0            1.0\n",
      "1223              0.0            1.0\n",
      "1224              0.0            1.0\n",
      "1225              0.0            1.0\n",
      "1226              1.0            0.0\n",
      "1227              0.0            1.0\n",
      "1228              1.0            0.0\n",
      "1229              0.0            1.0\n",
      "1230              0.0            1.0\n",
      "1231              1.0            0.0\n",
      "1232              0.0            1.0\n",
      "1233              1.0            0.0\n",
      "1234              1.0            0.0\n",
      "1235              0.0            1.0\n",
      "1236              0.0            1.0\n",
      "1237              0.0            1.0\n",
      "1238              0.0            1.0\n",
      "1239              1.0            0.0\n",
      "1240              1.0            0.0\n",
      "1241              1.0            0.0\n",
      "1242              1.0            0.0\n",
      "1243              0.0            1.0\n",
      "1244              0.0            1.0\n",
      "1245              0.0            1.0\n",
      "1246              1.0            0.0\n",
      "1247              0.0            1.0\n",
      "1248              1.0            0.0\n",
      "1249              1.0            0.0\n",
      "\n",
      "[1250 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "y, X = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket, return_type = 'dataframe')\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we are more interested in stock marketing up, we take the second column of y as our response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>Direction[Up]</td>  <th>  No. Observations:  </th>  <td>  1250</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1243</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Mon, 27 Feb 2017</td> <th>  Pseudo R-squ.:     </th> <td>0.002074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:05:16</td>     <th>  Log-Likelihood:    </th> <td> -863.79</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -865.59</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td>0.7319</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.1260</td> <td>    0.241</td> <td>   -0.523</td> <td> 0.601</td> <td>   -0.598     0.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag1</th>      <td>   -0.0731</td> <td>    0.050</td> <td>   -1.457</td> <td> 0.145</td> <td>   -0.171     0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag2</th>      <td>   -0.0423</td> <td>    0.050</td> <td>   -0.845</td> <td> 0.398</td> <td>   -0.140     0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag3</th>      <td>    0.0111</td> <td>    0.050</td> <td>    0.222</td> <td> 0.824</td> <td>   -0.087     0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag4</th>      <td>    0.0094</td> <td>    0.050</td> <td>    0.187</td> <td> 0.851</td> <td>   -0.089     0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag5</th>      <td>    0.0103</td> <td>    0.050</td> <td>    0.208</td> <td> 0.835</td> <td>   -0.087     0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Volume</th>    <td>    0.1354</td> <td>    0.158</td> <td>    0.855</td> <td> 0.392</td> <td>   -0.175     0.446</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          Direction[Up]   No. Observations:                 1250\n",
       "Model:                          Logit   Df Residuals:                     1243\n",
       "Method:                           MLE   Df Model:                            6\n",
       "Date:                Mon, 27 Feb 2017   Pseudo R-squ.:                0.002074\n",
       "Time:                        13:05:16   Log-Likelihood:                -863.79\n",
       "converged:                       True   LL-Null:                       -865.59\n",
       "                                        LLR p-value:                    0.7319\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.1260      0.241     -0.523      0.601        -0.598     0.346\n",
       "Lag1          -0.0731      0.050     -1.457      0.145        -0.171     0.025\n",
       "Lag2          -0.0423      0.050     -0.845      0.398        -0.140     0.056\n",
       "Lag3           0.0111      0.050      0.222      0.824        -0.087     0.109\n",
       "Lag4           0.0094      0.050      0.187      0.851        -0.089     0.107\n",
       "Lag5           0.0103      0.050      0.208      0.835        -0.087     0.107\n",
       "Volume         0.1354      0.158      0.855      0.392        -0.175     0.446\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = sm.Logit(y.ix[:,1], X)\n",
    "logit.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To extract the parameters directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Intercept   -0.126000\n",
       "Lag1        -0.073074\n",
       "Lag2        -0.042301\n",
       "Lag3         0.011085\n",
       "Lag4         0.009359\n",
       "Lag5         0.010313\n",
       "Volume       0.135441\n",
       "dtype: float64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit().params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To extract the probability of the market going up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.50708413,  0.48146788,  0.48113883,  0.51522236,  0.51078116,\n",
       "        0.50695646,  0.49265087,  0.50922916,  0.51761353,  0.48883778,\n",
       "        0.4965211 ])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit().predict()[0:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up (1) or Down (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label = pd.DataFrame(np.zeros(shape=(1250,1)), columns = ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    }
   ],
   "source": [
    "predict_label.ix[logit.fit().predict()>0.5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can evalue the TRAINING result by constructing a confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145, 457],\n",
       "       [141, 507]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y.ix[:,1], predict_label.ix[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. In this case, logistic regression correctly predicted the movement of the market 52.2% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52159999999999995"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y.ix[:,1] == predict_label.ix[:,0]) # to get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data. This will yield a more realistic error rate, in the sense that in practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Smarket_2005 = Smarket.query('Year >= 2005')\n",
    "Smarket_train = Smarket.query('Year < 2005')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the training dataset to build the logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_2005, return_type = 'dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          Direction[Up]   No. Observations:                  998\n",
      "Model:                          Logit   Df Residuals:                      991\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Mon, 27 Feb 2017   Pseudo R-squ.:                0.001562\n",
      "Time:                        13:05:17   Log-Likelihood:                -690.55\n",
      "converged:                       True   LL-Null:                       -691.63\n",
      "                                        LLR p-value:                    0.9044\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.1912      0.334      0.573      0.567        -0.463     0.845\n",
      "Lag1          -0.0542      0.052     -1.046      0.295        -0.156     0.047\n",
      "Lag2          -0.0458      0.052     -0.884      0.377        -0.147     0.056\n",
      "Lag3           0.0072      0.052      0.139      0.889        -0.094     0.108\n",
      "Lag4           0.0064      0.052      0.125      0.901        -0.095     0.108\n",
      "Lag5          -0.0042      0.051     -0.083      0.934        -0.104     0.096\n",
      "Volume        -0.1163      0.240     -0.485      0.628        -0.586     0.353\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "logit = sm.Logit(y_train.ix[:,1], X_train)\n",
    "print logit.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[77, 34],\n",
       "       [97, 44]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.5\n",
    "predict_label.ix[preds >threshold] = 1\n",
    "confusion_matrix(y_test.ix[:,1], predict_label.ix[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48015873015873017"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test.ix[:,1]==predict_label.ix[:,0]) # to get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period. The results are rather disappointing: the test error rate is 1 - 48% = 52 %, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The retrain of the model with Lag1 and Lag2 will be similar to previous steps (I will omit those). Another way to deal with logistics regression is to change the threshold value from 0.5 to others. There is an example below with threshold 0.45. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.56746031746031744"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.45\n",
    "predict_label.ix[preds >threshold] = 1\n",
    "confusion_matrix(y_test.ix[:,1], predict_label.ix[:,0])\n",
    "np.mean(y_test.ix[:,1]==predict_label.ix[:,0]) # to get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692085\n",
      "         Iterations 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55952380952380953"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2', Smarket_2005, return_type = 'dataframe')\n",
    "logit = sm.Logit(y_train.ix[:,1], X_train)\n",
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.5\n",
    "predict_label.ix[preds >threshold] = 1\n",
    "np.mean(y_test.ix[:,1]==predict_label.ix[:,0]) # to get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.3 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use sklearn's implementation of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn_lda = LDA(n_components=2) #creating a LDA object\n",
    "lda = sklearn_lda.fit(X_train.ix[:,1:3], y_train.ix[:,1]) #learning the projection matrix\n",
    "X_lda = lda.transform(X_train.ix[:,1:3]) #using the model to project X \n",
    "X_labels = lda.predict(X_train.ix[:,1:3]) #gives you the predicted label for each sample\n",
    "X_prob = lda.predict_proba(X_train.ix[:,1:3]) #the probability of each sample to belong to each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_labels=lda.predict(X_test.ix[:,1:3])\n",
    "X_test_prob = lda.predict_proba(X_test.ix[:,1:3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the accuracy of the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55952380952380953"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test.ix[:,1]==X_test_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's change the threshod a bit to see whether we can improve the accuracy. The 2nd column of X_test_prob is the probability belongs to UP group. The default value is 0.5, let us first check that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55952380952380953"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5 \n",
    "np.mean(y_test.ix[:,1]==(X_test_prob[:,1]>=threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56349206349206349"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.48\n",
    "np.mean(y_test.ix[:,1]==(X_test_prob[:,1]>=threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is a little bit of annoying that QDA and LDA have minor difference in their parameter set-up and function names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.599206349206\n"
     ]
    }
   ],
   "source": [
    "sklearn_qda = QDA(priors=None,store_covariances=True) #creating a QDA object\n",
    "qda = sklearn_qda.fit(X_train.ix[:,1:3], y_train.ix[:,1]) #learning the projection matrix\n",
    "X_labels = qda.predict(X_train.ix[:,1:3]) #gives you the predicted label for each sample\n",
    "X_prob = qda.predict_proba(X_train.ix[:,1:3]) #the probability of each sample to belong to each class\n",
    "\n",
    "X_test_labels=qda.predict(X_test.ix[:,1:3])\n",
    "X_test_prob = qda.predict_proba(X_test.ix[:,1:3]) \n",
    "\n",
    "print np.mean(y_test.ix[:,1]==X_test_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, use dir() to explore all the information stored in lda and qda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_decision_function',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " 'classes_',\n",
       " 'covariances_',\n",
       " 'decision_function',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'means_',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'priors',\n",
       " 'priors_',\n",
       " 'reg_param',\n",
       " 'rotations_',\n",
       " 'scalings_',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'store_covariances',\n",
       " 'tol']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n",
      "[array([[ 1.50662277, -0.03924806],\n",
      "       [-0.03924806,  1.53559498]]), array([[ 1.51700576, -0.02787349],\n",
      "       [-0.02787349,  1.49026815]])]\n"
     ]
    }
   ],
   "source": [
    "print qda.means_\n",
    "print qda.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.5 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.531746031746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_estimator_type',\n",
       " '_fit',\n",
       " '_fit_X',\n",
       " '_fit_method',\n",
       " '_get_param_names',\n",
       " '_init_params',\n",
       " '_pairwise',\n",
       " '_tree',\n",
       " '_y',\n",
       " 'algorithm',\n",
       " 'classes_',\n",
       " 'effective_metric_',\n",
       " 'effective_metric_params_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'kneighbors',\n",
       " 'kneighbors_graph',\n",
       " 'leaf_size',\n",
       " 'metric',\n",
       " 'metric_params',\n",
       " 'n_jobs',\n",
       " 'n_neighbors',\n",
       " 'outputs_2d_',\n",
       " 'p',\n",
       " 'predict',\n",
       " 'predict_proba',\n",
       " 'radius',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'weights']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNN(n_neighbors= 3) # use n_neighbors to change the # of tune the performance of KNN\n",
    "KNN_fit = neigh.fit(X_train.ix[:,1:3], y_train.ix[:,1]) #learning the projection matrix\n",
    "X_test_labels=KNN_fit.predict(X_test.ix[:,1:3])\n",
    "X_test_prob = KNN_fit.predict_proba(X_test.ix[:,1:3]) \n",
    "\n",
    "print np.mean(y_test.ix[:,1]==X_test_labels) \n",
    "\n",
    "dir(neigh) # use dir command to check what KNN offers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.6 An Application to Caravan Insurance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Caravan = pd.read_csv('data/Caravan.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5822, 86)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caravan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOSTYPE</th>\n",
       "      <th>MAANTHUI</th>\n",
       "      <th>MGEMOMV</th>\n",
       "      <th>MGEMLEEF</th>\n",
       "      <th>MOSHOOFD</th>\n",
       "      <th>MGODRK</th>\n",
       "      <th>MGODPR</th>\n",
       "      <th>MGODOV</th>\n",
       "      <th>MGODGE</th>\n",
       "      <th>MRELGE</th>\n",
       "      <th>...</th>\n",
       "      <th>APERSONG</th>\n",
       "      <th>AGEZONG</th>\n",
       "      <th>AWAOREG</th>\n",
       "      <th>ABRAND</th>\n",
       "      <th>AZEILPL</th>\n",
       "      <th>APLEZIER</th>\n",
       "      <th>AFIETS</th>\n",
       "      <th>AINBOED</th>\n",
       "      <th>ABYSTAND</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MOSTYPE  MAANTHUI  MGEMOMV  MGEMLEEF  MOSHOOFD  MGODRK  MGODPR  MGODOV  \\\n",
       "0       33         1        3         2         8       0       5       1   \n",
       "1       37         1        2         2         8       1       4       1   \n",
       "2       37         1        2         2         8       0       4       2   \n",
       "3        9         1        3         3         3       2       3       2   \n",
       "4       40         1        4         2        10       1       4       1   \n",
       "\n",
       "   MGODGE  MRELGE    ...     APERSONG  AGEZONG  AWAOREG  ABRAND  AZEILPL  \\\n",
       "0       3       7    ...            0        0        0       1        0   \n",
       "1       4       6    ...            0        0        0       1        0   \n",
       "2       4       3    ...            0        0        0       1        0   \n",
       "3       4       5    ...            0        0        0       1        0   \n",
       "4       4       7    ...            0        0        0       1        0   \n",
       "\n",
       "   APLEZIER  AFIETS  AINBOED  ABYSTAND  Purchase  \n",
       "0         0       0        0         0        No  \n",
       "1         0       0        0         0        No  \n",
       "2         0       0        0         0        No  \n",
       "3         0       0        0         0        No  \n",
       "4         0       0        0         0        No  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caravan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOSTYPE</th>\n",
       "      <th>MAANTHUI</th>\n",
       "      <th>MGEMOMV</th>\n",
       "      <th>MGEMLEEF</th>\n",
       "      <th>MOSHOOFD</th>\n",
       "      <th>MGODRK</th>\n",
       "      <th>MGODPR</th>\n",
       "      <th>MGODOV</th>\n",
       "      <th>MGODGE</th>\n",
       "      <th>MRELGE</th>\n",
       "      <th>...</th>\n",
       "      <th>ALEVEN</th>\n",
       "      <th>APERSONG</th>\n",
       "      <th>AGEZONG</th>\n",
       "      <th>AWAOREG</th>\n",
       "      <th>ABRAND</th>\n",
       "      <th>AZEILPL</th>\n",
       "      <th>APLEZIER</th>\n",
       "      <th>AFIETS</th>\n",
       "      <th>AINBOED</th>\n",
       "      <th>ABYSTAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.253349</td>\n",
       "      <td>1.110615</td>\n",
       "      <td>2.678805</td>\n",
       "      <td>2.991240</td>\n",
       "      <td>5.773617</td>\n",
       "      <td>0.696496</td>\n",
       "      <td>4.626932</td>\n",
       "      <td>1.069907</td>\n",
       "      <td>3.258502</td>\n",
       "      <td>6.183442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076606</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>0.570079</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.031776</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.014256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.846706</td>\n",
       "      <td>0.405842</td>\n",
       "      <td>0.789835</td>\n",
       "      <td>0.814589</td>\n",
       "      <td>2.856760</td>\n",
       "      <td>1.003234</td>\n",
       "      <td>1.715843</td>\n",
       "      <td>1.017503</td>\n",
       "      <td>1.597647</td>\n",
       "      <td>1.909482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377569</td>\n",
       "      <td>0.072782</td>\n",
       "      <td>0.080532</td>\n",
       "      <td>0.077403</td>\n",
       "      <td>0.562058</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.081632</td>\n",
       "      <td>0.210986</td>\n",
       "      <td>0.090463</td>\n",
       "      <td>0.119996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MOSTYPE     MAANTHUI      MGEMOMV     MGEMLEEF     MOSHOOFD  \\\n",
       "count  5822.000000  5822.000000  5822.000000  5822.000000  5822.000000   \n",
       "mean     24.253349     1.110615     2.678805     2.991240     5.773617   \n",
       "std      12.846706     0.405842     0.789835     0.814589     2.856760   \n",
       "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "25%      10.000000     1.000000     2.000000     2.000000     3.000000   \n",
       "50%      30.000000     1.000000     3.000000     3.000000     7.000000   \n",
       "75%      35.000000     1.000000     3.000000     3.000000     8.000000   \n",
       "max      41.000000    10.000000     5.000000     6.000000    10.000000   \n",
       "\n",
       "            MGODRK       MGODPR       MGODOV       MGODGE       MRELGE  \\\n",
       "count  5822.000000  5822.000000  5822.000000  5822.000000  5822.000000   \n",
       "mean      0.696496     4.626932     1.069907     3.258502     6.183442   \n",
       "std       1.003234     1.715843     1.017503     1.597647     1.909482   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     4.000000     0.000000     2.000000     5.000000   \n",
       "50%       0.000000     5.000000     1.000000     3.000000     6.000000   \n",
       "75%       1.000000     6.000000     2.000000     4.000000     7.000000   \n",
       "max       9.000000     9.000000     5.000000     9.000000     9.000000   \n",
       "\n",
       "          ...            ALEVEN     APERSONG      AGEZONG      AWAOREG  \\\n",
       "count     ...       5822.000000  5822.000000  5822.000000  5822.000000   \n",
       "mean      ...          0.076606     0.005325     0.006527     0.004638   \n",
       "std       ...          0.377569     0.072782     0.080532     0.077403   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "max       ...          8.000000     1.000000     1.000000     2.000000   \n",
       "\n",
       "            ABRAND      AZEILPL     APLEZIER       AFIETS      AINBOED  \\\n",
       "count  5822.000000  5822.000000  5822.000000  5822.000000  5822.000000   \n",
       "mean      0.570079     0.000515     0.006012     0.031776     0.007901   \n",
       "std       0.562058     0.022696     0.081632     0.210986     0.090463   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       7.000000     1.000000     2.000000     3.000000     2.000000   \n",
       "\n",
       "          ABYSTAND  \n",
       "count  5822.000000  \n",
       "mean      0.014256  \n",
       "std       0.119996  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       2.000000  \n",
       "\n",
       "[8 rows x 85 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caravan.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale of the variables matters in KNN ! The core question in KNN is how to define proper distance metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of 1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of 1, 000 is quite small compared to an age difference of 50 years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A good way (?) to handle this problem is to standardize the data so that all standardize variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just scale() this. In standardizing the data, we exclude column 86, because that is the qualitative Purchase variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label = pd.DataFrame(np.zeros(shape=(Caravan.shape[0],1)), columns = ['label'])\n",
    "predict_label.ix[Caravan['Purchase'] == 'Yes'] = 1\n",
    "Caravan_drop = Caravan.drop(labels='Purchase', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I took a slightly different approach from the book. The training and testing data were splited by index. The normalization was done on the train set. Afterwards, the same normalization was applied to validate test.  The code might seem wordy, but it helps clear the logical flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "train_size = 1000\n",
    "train_index = xrange(0, train_size)\n",
    "X_validate = Caravan_drop.ix[train_index, ]\n",
    "Y_validate = predict_label.ix[train_index, ]\n",
    "X_train = Caravan_drop.ix[train_size:, ]\n",
    "Y_train = predict_label.ix[train_size:, ]\n",
    "\n",
    "\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_validate_scaled = scaler.transform(X_validate)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with 1 neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.883\n",
      "[[874  67]\n",
      " [ 50   9]]\n"
     ]
    }
   ],
   "source": [
    "neigh = KNN(n_neighbors= 1) # use n_neighbors to change the # of tune the performance of KNN\n",
    "KNN_fit = neigh.fit(X_train_scaled, Y_train.ix[:,0]) #learning the projection matrix\n",
    "X_validate_labels=KNN_fit.predict(X_validate_scaled)\n",
    "X_validate_prob = KNN_fit.predict_proba(X_validate_scaled) \n",
    "\n",
    "print np.mean(Y_validate.ix[:,0]==X_validate_labels) \n",
    "print confusion_matrix(Y_validate.ix[:,0], X_validate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The rest of this exercise considers all the trade-off between False postive and False negative.  The concept of accuracy is NOT always the golden metric for classification problems. \n",
    "### Precision and recall, sensitivity and specificity, F1 score... are all reasonable metrics to consider. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
