{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.9 Lab: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve, auc, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import json\n",
    "import tensorflow.keras as tk\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "In Python, we use keras as the DL interface with backend Tensorflow.\n",
    "pip install keras \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9.1 A Single Layer Network on the Hitters Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, we will use Hitters data set to predict the salary of a player\n",
    "# I will skip the linear regression and Lasso part since we covered them in previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data and take a look at the data and split it into train and test \n",
    "# I copied the code from chapter (because of laziness :- )\n",
    "Hitters = pd.read_csv('data/Hitters.csv', header=0, na_values='NA')\n",
    "Hitters = Hitters.dropna().reset_index(drop=True) # drop the observation with NA values and reindex the obs from 0\n",
    "dummies = pd.get_dummies(Hitters[['League', 'Division', 'NewLeague']])\n",
    "\n",
    "y = Hitters.Salary  # the response variable \n",
    "X_prep = Hitters.drop (['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "X = pd.concat([X_prep,  dummies[['League_A', 'Division_E', 'NewLeague_A']]], axis=1)\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.66)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the DL model is similar to other models implemeted in sklearn. \n",
    "# we first define the model, then fit the model, and finally predict the result\n",
    "\n",
    "\"\"\"\n",
    "I am listing out the hyperameters to be tuned. \n",
    "From this simple example with only one layer, we could get a sense of the number of hyperameters in NN.\n",
    "\n",
    "Actually the number of the hyperameters gets exponentially larger as the number of layers increases.\n",
    "\"\"\"\n",
    "# define the model.model.add\n",
    "dropout_rate = 0.4\n",
    "first_layout = 50\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "activation = 'relu'\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'rmsprop'\n",
    "metrics = ['mae']\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dropout(rate=dropout_rate, input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(first_layout, activation=activation))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "# we can use the model.summary() to see the structure of the model\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "# model.evaluate returns the loss value & metrics values for the model in test mode.\n",
    "mse_test, mae_test = model.evaluate(X_test, y_test)\n",
    "print('Test mse: %.3f, Test mae: %.3f' % (mse_test, mae_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9.2 A Multilayer Network on the MNIST Digit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could load the MNIST data set from keras.datasets\n",
    "# keras.datasets also contains other well-known datasets, such as cifar10, fashion_mnist, etc.\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the data set if for image, so each image is a 28*28 matrix.\n",
    "print(X_train.shape)\n",
    "print(np.max(X_train))\n",
    "\n",
    "# the y_train the group label for the training data\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us plot some of the images\n",
    "for i in range(9):\n",
    "\t# define subplot\n",
    "\tplt.subplot(330 + 1 + i)\n",
    "\tplt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us reshape each image (i.e. matrix) to a vector\n",
    "X_train = X_train.reshape((X_train.shape[0], 28*28)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], 28*28)).astype('float32')\n",
    "\"\"\" \n",
    "We know that each pixel has its unique color code and also we know that it has a maximum value of 255. \n",
    "To perform Machine Learning, it is important to convert all the values from 0 to 255 for every pixel to \n",
    "a range of values from 0 to 1. The simplest way is to divide the value of every pixel by 255 to get the \n",
    "values in the range of 0 to 1.\n",
    "\"\"\"\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim = 28 * 28, activation= 'relu'))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model and fit the model \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_model = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot the metric history \n",
    "history_dict = history_model.history\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history_dict['accuracy'], color='black', label='train')\n",
    "plt.plot(history_dict['val_accuracy'], color='orange', label='test')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\"\"\" \n",
    "One thing to notice is the test accuracy is actually higher than the training accuracy. \n",
    "It is kind of uncommon - if you have a good explaination on this, let me know. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9.3 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tk.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the data shape, compare to MNIST, this dataset is also image but with \n",
    "# different channels, i.e. squared image with 32Ã—32 pixels and three color channels\n",
    "print(X_train.shape)\n",
    "print(np.max(X_train))\n",
    "\n",
    "# the y_train the group label for the training data\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few samples\n",
    "for i in range(9):\n",
    "\t# define subplot\n",
    "\tplt.subplot(330 + 1 + i)\n",
    "\tplt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the workflow is similar to before\n",
    "# we prepare the data \n",
    "# then define the model architecture \n",
    "# then fit the model \n",
    "# then exam the model performance \n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a 3 block (each block contains conv layer and pooling layer) VGG architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# then define the output classifer part of the model \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(100, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model and fit the model \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_model = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test), verbose=0)\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot the metric history \n",
    "history_dict = history_model.history\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history_dict['accuracy'], color='black', label='train')\n",
    "plt.plot(history_dict['val_accuracy'], color='orange', label='test')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as before, we can add in dropout \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "# compile the model and fit the model \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_model = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test), verbose=0)\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: %.3f' % acc)\n",
    "\"\"\" \n",
    "From the result, we can see that adding in dropout decreases the test accuracy from \n",
    "37.6% to 34.1%. This is mainly due to the underfit of the model.\n",
    "\n",
    "Dropout helps in the case of overfitting, to see the benefit of the dropout, we would need to \n",
    "increase epochs to a large number (~ 1000). \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some other regularization technique is not mentioned in this lab is weight decay (i.e. similar to the concept of Lasso and Ridge)\n",
    "In this setup, we can add regularization to the weights using the syntax 'kernel_regularizer'\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "Some other regularization methods are data augmentation and early stopping. \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4548a0e672c5b3a287feee7b2962606840aa548749d1830ef724408652b0c250"
  },
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
