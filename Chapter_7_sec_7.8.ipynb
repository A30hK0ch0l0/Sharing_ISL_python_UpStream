{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.8 Lab: Non-linear Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this lab, we will use Wage data. Let us read in the CSV data ans look at a sample of this data.\n",
    "\"\"\"\n",
    "Wage = pd.read_csv('data/Wage.csv', header=0, na_values='NA')\n",
    "print Wage.shape\n",
    "Wage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8.1 Polynomial Regression and Step Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will examine how to fit a polynomial regression model on the wage dataset. As all the techniques, we have multiple ways to do this. Here I will use sklearn as we alreadly used statsmodel.api before in Chapter 3.  If you are looking for more built-in functions around p-value, significance, confidence intervie, etc., I would recommend to use statsmodel.api. \n",
    "\n",
    "But scikit-learn does not have built error estimates for doing inference. But this problem forces us to think about a more general method to find Confidence Interview (key word: Bootstrap) \n",
    "\n",
    "Numpy also has a nice function to do ploynomial regression: https://www.ritchieng.com/machine-learning-polynomial-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_deg = 4\n",
    "X = Wage.age\n",
    "y = Wage.wage\n",
    "X = X.reshape(X.shape[0], 1)\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "\n",
    "polynomial_features= PolynomialFeatures(degree=n_deg)\n",
    "X_poly = polynomial_features.fit_transform(X)\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_poly, y)\n",
    "\n",
    "# get coefficients and compare with the numbers as the end of page 288.\n",
    "print reg.intercept_, reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a grid of values for age at which we want predictionsm and the call the generic predict() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of age values spanning the range\n",
    "age_grid = np.arange(Wage.age.min(), Wage.age.max()).reshape(-1,1)\n",
    "\n",
    "# generate test data use PolynomialFeatures and fit_transform\n",
    "X_test = PolynomialFeatures(degree=n_deg).fit_transform(age_grid)\n",
    "\n",
    "# predict the value of the generated ages\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# creating plots\n",
    "plt.plot(age_grid, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide on the polynomial to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the book, the authors did this by using hypothesis testing. ANOVA using F-test was explanied. In order\n",
    "to use the ANOVA function, $M_1$ and $M_2$ must be nested model: the predictors in $M_1$ must be a subset of the predictors in $M_2$. statsmodel.api has a nice built-in function to do that. \n",
    "\n",
    "As an alternative to using hypothesis tests and ANOVA, we could choose the polynomial degree using cross-validation, as discussed in before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = PolynomialFeatures(1).fit_transform(X)\n",
    "X2 = PolynomialFeatures(2).fit_transform(X)\n",
    "X3 = PolynomialFeatures(3).fit_transform(X)\n",
    "X4 = PolynomialFeatures(4).fit_transform(X)\n",
    "X5 = PolynomialFeatures(5).fit_transform(X)\n",
    "fit1 = sm.GLS(y, X1).fit()\n",
    "fit2 = sm.GLS(y, X2).fit()\n",
    "fit3 = sm.GLS(y, X3).fit()\n",
    "fit4 = sm.GLS(y, X4).fit()\n",
    "fit5 = sm.GLS(y, X5).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "print(sm.stats.anova_lm(fit1, fit2, fit3, fit4, fit5, typ=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row of the above take shows the fit1 to the quadratic model fit2 is $2.36*10^{-32}$, indicating that a quadratic model is significant informative to a linear model. Similarly, the cubic model is significnat informative to a quadratic model ($p = 1.68 * 10^{-2}$).Hence, either a cubic or a quartic polynomial appear to provide a reasonable fit to the data, but lower- or higher-order models are not justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the book, the authors also discussed logistic regression and the polynomial terms. In python, sm.GLM function provided some functions similar to glm() in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = sm.GLM ((y>250), X4, family=sm.families.Binomial())\n",
    "logistic_fit = logistic_model.fit()\n",
    "print(logistic_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit a step function, we use the cut() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cut, bins = pd.cut(Wage.age, bins=4, retbins=True, right=True)\n",
    "age_cut.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here cut() automatically picked the cutpoints at 33.5, 49, and 64.5 years of age. We could also have specified our own cutpoints directly using the breaks option  (set bins into a sequence of scalars, e.g. [0, 10, 20, 40, 100]). Note in the following code, I manually added a constant column and dropped the lowest value bin (17.938, 33.5] dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cut_dummies = pd.get_dummies(age_cut)\n",
    "age_cut_dummies = sm.add_constant(age_cut_dummies)\n",
    "fit_age_cut = sm.GLM(Wage.wage, age_cut_dummies.drop(age_cut_dummies.columns[1], axis=1)).fit()\n",
    "print(fit_age_cut.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
